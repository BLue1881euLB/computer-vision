{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "from matplotlib.pyplot import Rectangle\n",
    "\n",
    "p = os.path.join(os.path.dirname('__file__'), '../..')\n",
    "sys.path.append(p)\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untar files into data directory\n",
    "# tar zxvf bbox_images.tar.gz\n",
    "DATA_DIR = '../../data/'\n",
    "IMG_DIR = os.path.join(DATA_DIR, 'volleyball', 'images_subset')\n",
    "metadata_fpath = os.path.join(DATA_DIR, 'volleyball_bbox_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls {DATA_DIR}\n",
    "%ls {IMG_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 4 bad files\n",
    "%rm {IMG_DIR}/._*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(arr, fs=(10,10), cmap='gray', title=None):\n",
    "    plt.figure(figsize=fs)\n",
    "    plt.imshow(arr, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def load_img(fpath):\n",
    "    return plt.imread(fpath)\n",
    "\n",
    "def load_cv2_img(fpath, w=None, h=None, colorspace=None):\n",
    "    img = cv2.imread(img_fpath)\n",
    "    if colorspace is not None:\n",
    "        img = cv2.cvtColor(img, colorspace)\n",
    "    if None not in [w,h]:\n",
    "        img = cv2.resize(img, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "    return img\n",
    "\n",
    "def make_boxes(meta):\n",
    "    boxes = {}\n",
    "    for idx,row in meta.iterrows():\n",
    "        box = json.loads(row.to_json())\n",
    "        fname = row['filename']\n",
    "        if fname in boxes:\n",
    "            boxes[fname].append(box)\n",
    "        else:\n",
    "            boxes[fname] = [box]\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(metadata_fpath)\n",
    "metadata['label_name'] = 'ball'\n",
    "metadata['label_id'] = 1\n",
    "fnames = metadata['filename']\n",
    "fpaths = [os.path.join(IMG_DIR, f) for f in fnames]\n",
    "metadata['fpath'] = fpaths\n",
    "GT_BOXES = make_boxes(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fpath = fpaths[random.randint(0,len(fpaths)-1)]\n",
    "img = load_img(img_fpath)\n",
    "img = load_cv2_img(img_fpath, 640, 360)\n",
    "plot_img(img, fs=(15,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colorspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we convert?\n",
    "\n",
    "\n",
    "Links\n",
    "* https://www.learnopencv.com/color-spaces-in-opencv-cpp-python/\n",
    "* https://docs.opencv.org/3.2.0/df/d9d/tutorial_py_colorspaces.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr_img = load_cv2_img(img_fpath)\n",
    "\n",
    "# Shape (w,h,c)\n",
    "print(bgr_img.shape)\n",
    "\n",
    "# Plot\n",
    "plot_img(bgr_img, fs=(10,10))\n",
    "\n",
    "# Channels\n",
    "b,g,r = bgr_img[:,:,0], bgr_img[:,:,1], bgr_img[:,:,2]\n",
    "\n",
    "plot_img(b, title='Blue')\n",
    "plot_img(g, title='Green')\n",
    "plot_img(r, title='Red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Additive - combines Red, Green, Blue values\n",
    "* 3 Channels correlated by amount of light hitting surface\n",
    "* Problems\n",
    "    * Mixes color (chrominance) )and intensity (luminance) information into a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Plot\n",
    "plot_img(rgb_img, fs=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hue (Dominant wavelength)\n",
    "* Saturation (Purity / shades of the color)\n",
    "* Value (Intensity)\n",
    "\n",
    "Pros\n",
    "* Only one channel needed to describe color (H)\n",
    "* Best for color thresholding (why?)\n",
    "* More robust to reflections on the floor\n",
    "\n",
    "Cons\n",
    "* Device dependent\n",
    "\n",
    "![HSV]](https://edoras.sdsu.edu/doc/matlab/toolbox/images/hsvcone.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Plot\n",
    "plot_img(hsv_img, fs=(10,10))\n",
    "\n",
    "# Channels (Hue, Saturation, Value)\n",
    "h,s,v = hsv_img[:,:,0], hsv_img[:,:,1], hsv_img[:,:,2]\n",
    "\n",
    "plot_img(h, title='Hue')\n",
    "plot_img(s, title='Saturation')\n",
    "plot_img(v, title='Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine HSV value of specific color\n",
    "\n",
    "blue = np.uint8([[[255,0,0 ]]])\n",
    "hsv_blue = cv2.cvtColor(blue, cv2.COLOR_BGR2HSV)\n",
    "print(\"Blue\", hsv_blue)\n",
    "\n",
    "green = np.uint8([[[0,255,0 ]]])\n",
    "hsv_green = cv2.cvtColor(green, cv2.COLOR_BGR2HSV)\n",
    "print(\"Green\", hsv_green)\n",
    "\n",
    "red = np.uint8([[[0,0,255]]])\n",
    "hsv_red = cv2.cvtColor(red, cv2.COLOR_BGR2HSV)\n",
    "print(\"Red\", hsv_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Thresholding\n",
    "\n",
    "* https://docs.opencv.org/3.2.0/df/d9d/tutorial_py_colorspaces.html\n",
    "* https://www.learnopencv.com/color-spaces-in-opencv-cpp-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find color of pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_of_pixel(fpath, x, y, colorspace='BGR'):\n",
    "    rgb_img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "    hsv_img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    rgb_colors = rgb_img[y,x,:]\n",
    "    hsv_colors = hsv_img[y,x,:]\n",
    "    bgr_colors = np.copy(rgb_colors[::-1])\n",
    "    print (\"BGR:\", bgr_colors)\n",
    "    print (\"RGB:\", rgb_colors)\n",
    "    print (\"HSV:\", hsv_colors)\n",
    "    \n",
    "    # Plot to visualize\n",
    "    img = np.copy(rgb_img)\n",
    "    img[y-5:y+5:,x-5:x+5,:] = 255\n",
    "    img[y,x,:] = 0\n",
    "    plot_img(img, fs=(18,18))\n",
    "    \n",
    "    if colorspace == 'BGR':\n",
    "        return bgr_colors.tolist()\n",
    "    if colorspace == 'RGB':\n",
    "        return rgb_colors.tolist()\n",
    "    return hsv_colors.tolist()\n",
    "\n",
    "get_color_of_pixel(img_fpath, 100, 100, 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hsv_value_of_bgr(bgr_color):\n",
    "    print(\"BGR\", bgr_color)\n",
    "    bgr_color = np.uint8([[bgr_color]])\n",
    "    hsv = cv2.cvtColor(bgr_color, cv2.COLOR_BGR2HSV)[0][0]\n",
    "    print(\"HSV\", hsv)\n",
    "    return hsv\n",
    "\n",
    "blue = [255,0,0 ]\n",
    "_ = get_hsv_value_of_bgr(blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr_img = load_cv2_img(img_fpath)\n",
    "\n",
    "# Cv2 Histogram (faster)\n",
    "hist = cv2.calcHist(images=[bgr_img], channels=[0], mask=None, histSize=[256], ranges=[0,256])\n",
    "\n",
    "# Numpy Histogram (slower)\n",
    "hist, bins = np.histogram(bgr_img.ravel(), 256, [0,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Histogram (all channels flattened)\n",
    "def plot_hist(img, bins=256, title=None):\n",
    "    plt.hist(img.ravel(), bins=bins, range=[0,256])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(bgr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bgr_hist(bgr_img, bins=256, mask=None):\n",
    "    # Mask let's you select for certain regions    \n",
    "    color = ('b','g','r')\n",
    "    for i,col in enumerate(color):\n",
    "        histr = cv2.calcHist([bgr_img],[i],mask,[bins],[0,256])\n",
    "        plt.plot(histr, color=col)\n",
    "        plt.xlim([0,bins])\n",
    "    plt.show()\n",
    "\n",
    "plot_bgr_hist(bgr_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(img, color, thresh):\n",
    "    \"\"\"\n",
    "    color = [b, g, r] or [r,b,g] or [h,s,v]\n",
    "    thresh = margin allowed around color\n",
    "    \"\"\"\n",
    "    min_color = np.array([color[0]-thresh, color[1]-thresh, color[2]-thresh])\n",
    "    max_color = np.array([color[0]+thresh, color[1]+thresh, color[2]+thresh])\n",
    "    min_color[min_color < 0] = 0\n",
    "    max_color[max_color > 255] = 255\n",
    "    print(\"Min\", min_color)\n",
    "    print(\"Max\", max_color)\n",
    "    \n",
    "    mask = cv2.inRange(img, min_color, max_color)\n",
    "    result = cv2.bitwise_and(img, img, mask=mask)\n",
    "    return mask, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGR Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the court\n",
    "bgr_color = get_color_of_pixel(img_fpath, 700, 650, 'BGR')\n",
    "bgr_img = load_cv2_img(img_fpath)\n",
    "mask, result = threshold(bgr_img, bgr_color, 35)\n",
    "plot_img(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight floor around court (darker green)\n",
    "bgr_color = get_color_of_pixel(img_fpath, 1100, 550, 'BGR')\n",
    "bgr_img = load_cv2_img(img_fpath)\n",
    "mask1, result1 = threshold(bgr_img, bgr_color, 60)\n",
    "plot_img(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight floor around court (lighter green reflection)\n",
    "bgr_color = get_color_of_pixel(img_fpath, 1130, 460, 'BGR')\n",
    "bgr_img = load_cv2_img(img_fpath)\n",
    "mask2, result2 = threshold(bgr_img, bgr_color, 50)\n",
    "plot_img(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSV Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_color = get_color_of_pixel(img_fpath, 700, 650, 'HSV')\n",
    "hsv_img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2HSV)\n",
    "mask, result = threshold(hsv_img, hsv_color, 35)\n",
    "plot_img(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_color = get_color_of_pixel(img_fpath, 1130, 460, 'HSV')\n",
    "hsv_img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2HSV)\n",
    "mask, result = threshold(hsv_img, hsv_color, 55)\n",
    "plot_img(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_color = get_color_of_pixel(img_fpath, 25, 550, 'HSV')\n",
    "hsv_img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2HSV)\n",
    "mask, result = threshold(hsv_img, hsv_color, 55)\n",
    "plot_img(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Driven Thresholding\n",
    "\n",
    "* https://gist.github.com/danielballan/ab5e28420ba1b24c5ad4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_fpath = os.path.join(DATA_DIR, 'volleyball_frame_00665.json')\n",
    "img_fpath = os.path.join(DATA_DIR, 'volleyball_frame_00665.png')\n",
    "\n",
    "BOX_COLORS = {\n",
    "    'referee': 'black',\n",
    "    'red_team': 'red',\n",
    "    'blue_team': 'blue',\n",
    "    'court-inner': 'green',\n",
    "    'court_outer': 'white',\n",
    "}\n",
    "\n",
    "def plot_bbs_from_rectLabel_annos(fpath):\n",
    "    bb_json = json.load(open(json_fpath, 'r'))\n",
    "    img_fpath = os.path.join(IMG_DIR, bb_json['filename'])\n",
    "    fig = plt.figure(figsize=(18,18))\n",
    "    axes = plt.axes([0, 0.03, 1, 0.97])\n",
    "    \n",
    "    img = plt.imread(img_fpath)\n",
    "    imgplot = axes.imshow(img)\n",
    "\n",
    "    for box in bb_json['objects']:\n",
    "        label = box['label']\n",
    "        color = BOX_COLORS[label]\n",
    "        coords = box['x_y_w_h']\n",
    "        bb = Rectangle(\n",
    "            (coords[0],coords[1]), \n",
    "            coords[2], coords[3],\n",
    "            fill=False,\n",
    "            edgecolor=color,\n",
    "            linewidth=2)\n",
    "        axes.add_patch(bb)\n",
    "        \n",
    "plot_bbs_from_rectLabel_annos(json_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_crops_from_rectLabel_bbs(img, json_fpath):\n",
    "    crops_dict = {}\n",
    "    bb_json = json.load(open(json_fpath, 'r'))\n",
    "    for box in bb_json['objects']:\n",
    "        label = box['label']\n",
    "        x,y,w,h = box['x_y_w_h']\n",
    "        crop = img[y:y+h,x:x+w,:]\n",
    "        if label not in crops_dict:\n",
    "            crops_dict[label] = []\n",
    "        crops_dict[label].append(crop)\n",
    "    return crops_dict\n",
    "\n",
    "bgr_img = load_cv2_img(img_fpath)\n",
    "crops = get_img_crops_from_rectLabel_bbs(bgr_img, json_fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for crop in crops['red_team']:\n",
    "    plot_img(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for crop in crops['court-inner']:\n",
    "    plot_img(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color histograms representing average values among samples\n",
    "def get_flattened_channels(imgs):\n",
    "    chans = np.empty(shape=(1,3))\n",
    "    for img in imgs:\n",
    "        h,w,c = img.shape\n",
    "        reshaped = img.reshape((h*w, c))\n",
    "        chans = np.concatenate([chans, reshaped], axis=0)\n",
    "        #print(reshaped.shape, chans.shape)\n",
    "    return chans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# BGR Histograms\n",
    "bgr_img = load_cv2_img(img_fpath)\n",
    "crops = get_img_crops_from_rectLabel_bbs(bgr_img, json_fpath)\n",
    "\n",
    "hists = {}\n",
    "for label in crops.keys():\n",
    "    chans = get_flattened_channels(crops[label])\n",
    "    bgr = ('b','g','r')\n",
    "    hists[label] = {\n",
    "        'b':None,\n",
    "        'g':None,\n",
    "        'r':None\n",
    "    }\n",
    "    for i in range(len(chans[0])):\n",
    "        hist, bins = np.histogram(chans[:,i], 50, [0,256])\n",
    "        hists[label][bgr[i]] = hist\n",
    "        plot_hist(chans[:,i], bins=50, title=label + ' ' + bgr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSV Histograms\n",
    "hsv_img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2HSV)\n",
    "crops = get_img_crops_from_rectLabel_bbs(hsv_img, json_fpath)\n",
    "\n",
    "hists = {}\n",
    "for label in crops.keys():\n",
    "    chans = get_flattened_channels(crops[label])\n",
    "    channels = ('h','s','v')\n",
    "    hists[label] = {c:None for c in bgr}\n",
    "    for i in range(len(chans[0])):\n",
    "        hist, bins = np.histogram(chans[:,i], 50, [0,256])\n",
    "        hists[label][bgr[i]] = hist\n",
    "        plot_hist(chans[:,i], bins=50, title=label + ' ' + channels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "court inner\n",
    "B = 75 - 125\n",
    "G = 90 - 135\n",
    "R = 175 - 220\n",
    "\"\"\"\n",
    "def threshold(img, color, thresh, sigma=1.0):\n",
    "    \"\"\"\n",
    "    color = [b, g, r] or [r,b,g] or [h,s,v]\n",
    "    thresh = [b,g,r] margin allowed around color (1 per channel)\n",
    "    \"\"\"\n",
    "    thresh = np.array(thresh) * sigma\n",
    "    min_color = np.array([color[0]-thresh[0], color[1]-thresh[1], color[2]-thresh[2]])\n",
    "    max_color = np.array([color[0]+thresh[0], color[1]+thresh[1], color[2]+thresh[2]])\n",
    "    min_color[min_color < 0] = 0\n",
    "    max_color[max_color > 255] = 255\n",
    "    print(\"Min\", min_color)\n",
    "    print(\"Max\", max_color)\n",
    "    \n",
    "    mask = cv2.inRange(img, min_color, max_color)\n",
    "    result = cv2.bitwise_and(img, img, mask=mask)\n",
    "    return mask, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB\n",
    "\n",
    "img_fpath = os.path.join(IMG_DIR, 'volleyball_frame_00665.png')\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "plot_img(img, fs=(14,14), title=\"Original\")\n",
    "\n",
    "r = 175 + 220 // 2\n",
    "g = 90 + 135 // 2\n",
    "b = 75 + 125 // 2\n",
    "color = (\n",
    "    r,g,b\n",
    ")\n",
    "thresh = (\n",
    "    220 - 175 // 2,\n",
    "    135 - 90 // 2,\n",
    "    125 - 75 // 2\n",
    ")\n",
    "mask, result = threshold(img, color, thresh, sigma=.9)\n",
    "plot_img(result, fs=(14,14), title=\"Thresholded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSV\n",
    "\n",
    "img_fpath = os.path.join(IMG_DIR, 'volleyball_frame_00665.png')\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2HSV)\n",
    "plot_img(img, fs=(14,14), title=\"Original\")\n",
    "\"\"\"\n",
    "H 0 - 10\n",
    "S = 100 - 150\n",
    "V = 175 - 220\n",
    "\"\"\"\n",
    "color = (\n",
    "    5,\n",
    "    50,\n",
    "    45\n",
    ")\n",
    "thresh = (\n",
    "    6,\n",
    "    256,\n",
    "    256\n",
    ")\n",
    "mask, result = threshold(img, color, thresh, sigma=1)\n",
    "plot_img(result, fs=(14,14), title=\"Thresholded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erosion and Dilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html\n",
    "* https://www.geeksforgeeks.org/erosion-dilation-images-using-opencv-python/\n",
    "* Morphological operation\n",
    "* Goals:\n",
    "    * Remove noise\n",
    "    * Join nearby elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erosion\n",
    "\n",
    "* Erodes away boundaries of objects\n",
    "* Kernel slides through and returns 1 only if all neighbors are also 1\n",
    "* Thickness or size of object decreases (boundary pixels are disgarded\n",
    "* Use cases\n",
    "    * Removing noise around edges\n",
    "    * Detaching two connected objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fpath = os.path.join(IMG_DIR, 'volleyball_frame_00665.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "plot_img(img, fs=(18,18), title=\"Original\")\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ERODE, (3,3))\n",
    "dilation = cv2.erode(img, kernel,iterations = 2)\n",
    "plot_img(dilation, fs=(18,18), title=\"Erosion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n",
    "\n",
    "* Opposite of erosion\n",
    "* Expands object around edges\n",
    "* Useful for joining broken parts of an image\n",
    "* Typically we erode first (to remove noise around edges), then dilate to increase size of object\n",
    "* Use cases\n",
    "    * Increasing side of object after erosion\n",
    "    * Joining broken parts of an object\n",
    "    \n",
    "![Ellipse](https://study.com/cimages/multimages/16/semimajax2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "plot_img(img, fs=(18,18), title=\"Original\")\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "dilation = cv2.dilate(img, kernel,iterations = 2)\n",
    "plot_img(dilation, fs=(18,18), title=\"Dilation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erosion + Dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "plot_img(img, fs=(14,14), title=\"Original\")\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "erosion = cv2.erode(img, kernel,iterations = 1)\n",
    "plot_img(erosion, fs=(14,14), title=\"Erosion\")\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "dilation = cv2.dilate(img, kernel,iterations = 1)\n",
    "plot_img(dilation, fs=(14,14), title=\"Dilation\")\n",
    "\n",
    "combined = cv2.dilate(erosion, kernel, iterations = 1)\n",
    "plot_img(combined, fs=(14,14), title=\"Erosion + Dilation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSV Erosion + Dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2HSV)\n",
    "plot_img(img, fs=(14,14), title=\"Original\")\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "erosion = cv2.erode(img, kernel,iterations = 3)\n",
    "plot_img(erosion, fs=(14,14), title=\"Erosion\")\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "dilation = cv2.dilate(img, kernel,iterations = 1)\n",
    "plot_img(dilation, fs=(14,14), title=\"Dilation\")\n",
    "\n",
    "combined = cv2.dilate(erosion, kernel, iterations = 1)\n",
    "plot_img(combined, fs=(14,14), title=\"Erosion + Dilation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canny Edge Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fpath = os.path.join(IMG_DIR, 'volleyball_frame_00665.png')\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "plot_img(img, fs=(14,14), title=\"Original\")\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, ksize=(5,5), sigmaX=3)\n",
    "plot_img(gray, fs=(14,14), title=\"Grayscale\")\n",
    "\n",
    "# use Canny edge detector to find edges in the image.  The thresholds determine how\n",
    "# weak or strong an edge will be detected.  These can be tweaked.\n",
    "lower_threshold = 50\n",
    "upper_threshold = 50\n",
    "edges = cv2.Canny(gray, lower_threshold, upper_threshold)\n",
    "plot_img(edges, fs=(14,14))\n",
    "print(edges.shape)\n",
    "\n",
    "# Mask top of image\n",
    "edges[:400,:] = 0\n",
    "edges[:,1150:] = 0\n",
    "plot_img(edges, fs=(14,14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erosion, Dilation, Gaussian, Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RGB\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "plot_img(img, fs=(14,14), title=\"Original\")\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "erosion = cv2.erode(img, kernel,iterations = 1)\n",
    "plot_img(erosion, fs=(14,14), title=\"Erosion\")\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4,4))\n",
    "dilation = cv2.dilate(img, kernel,iterations = 2)\n",
    "plot_img(dilation, fs=(14,14), title=\"Dilation\")\n",
    "\n",
    "img = cv2.dilate(erosion, kernel, iterations = 2)\n",
    "plot_img(img, fs=(14,14), title=\"Erosion + Dilation\")\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, ksize=(5,5), sigmaX=3)\n",
    "plot_img(gray, fs=(14,14), title=\"Grayscale\")\n",
    "\n",
    "# use Canny edge detector to find edges in the image.  The thresholds determine how\n",
    "# weak or strong an edge will be detected.  These can be tweaked.\n",
    "lower_threshold = 25\n",
    "upper_threshold = 25\n",
    "edges = cv2.Canny(gray, lower_threshold, upper_threshold)\n",
    "plot_img(edges, fs=(14,14))\n",
    "\n",
    "# Mask top of image\n",
    "edges[:425,:] = 0\n",
    "edges[700:,:] = 0\n",
    "edges[:,1150:] = 0\n",
    "plot_img(edges, fs=(14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hough Transform - Lines\n",
    "\n",
    "# detect lines in the image.  This is where the real work is done.  Higher threshold\n",
    "# means a line needs to be stronger to be detected, so again, this can be tweaked.\n",
    "thresh = 125\n",
    "lines = cv2.HoughLines(edges, 1, np.pi / 180, thresh)\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# convert each line to coordinates back in the original image\n",
    "for line in lines:\n",
    "    for rho, theta in line:\n",
    "        a = np.cos(theta)\n",
    "        b = np.sin(theta)\n",
    "        x0 = a * rho\n",
    "        y0 = b * rho\n",
    "        x1 = int(x0 + 1000 * -b)\n",
    "        y1 = int(y0 + 1000 * a)\n",
    "        x2 = int(x0 - 1000 * -b)\n",
    "        y2 = int(y0 - 1000 * a)\n",
    "\n",
    "        # draw each line on the image\n",
    "        cv2.line(img, pt1=(x1, y1), pt2=(x2, y2), color=(0, 0, 255), thickness=3)\n",
    "\n",
    "# write the image to disk\n",
    "plot_img(img, fs=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSV - Erosion, Dilation, Blur, Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# HSV\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2HSV)\n",
    "plot_img(img, fs=(14,14), title=\"Original\")\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "erosion = cv2.erode(img, kernel,iterations = 1)\n",
    "plot_img(erosion, fs=(14,14), title=\"Erosion\")\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "dilation = cv2.dilate(img, kernel,iterations = 2)\n",
    "plot_img(dilation, fs=(14,14), title=\"Dilation\")\n",
    "\n",
    "img = cv2.dilate(img, kernel, iterations = 2)\n",
    "plot_img(img, fs=(14,14), title=\"Erosion + Dilation\")\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, ksize=(5,5), sigmaX=3)\n",
    "plot_img(gray, fs=(14,14), title=\"Grayscale\")\n",
    "\n",
    "# use Canny edge detector to find edges in the image.  The thresholds determine how\n",
    "# weak or strong an edge will be detected.  These can be tweaked.\n",
    "lower_threshold = 25\n",
    "upper_threshold = 125\n",
    "edges = cv2.Canny(gray, lower_threshold, upper_threshold)\n",
    "plot_img(edges, fs=(14,14))\n",
    "\n",
    "# Mask top of image\n",
    "edges[:425,:] = 0\n",
    "edges[700:,:] = 0\n",
    "edges[:,1150:] = 0\n",
    "plot_img(edges, fs=(14,14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hough Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hough Transform - Lines\n",
    "\n",
    "# detect lines in the image.  This is where the real work is done.  Higher threshold\n",
    "# means a line needs to be stronger to be detected, so again, this can be tweaked.\n",
    "thresh = 175\n",
    "lines = cv2.HoughLines(edges, 1, np.pi / 180, thresh)\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# convert each line to coordinates back in the original image\n",
    "for line in lines:\n",
    "    for rho, theta in line:\n",
    "        a = np.cos(theta)\n",
    "        b = np.sin(theta)\n",
    "        x0 = a * rho\n",
    "        y0 = b * rho\n",
    "        x1 = int(x0 + 1000 * -b)\n",
    "        y1 = int(y0 + 1000 * a)\n",
    "        x2 = int(x0 - 1000 * -b)\n",
    "        y2 = int(y0 - 1000 * a)\n",
    "\n",
    "        # draw each line on the image\n",
    "        cv2.line(img, pt1=(x1, y1), pt2=(x2, y2), color=(0, 0, 255), thickness=3)\n",
    "\n",
    "# write the image to disk\n",
    "plot_img(img, fs=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB, Color Thresh, Canny, Hough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fpath = os.path.join(IMG_DIR, 'volleyball_frame_00665.png')\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "plot_img(img, fs=(14,14), title=\"Original\")\n",
    "\n",
    "r = 175 + 220 // 2\n",
    "g = 90 + 135 // 2\n",
    "b = 75 + 125 // 2\n",
    "color = (\n",
    "    r,g,b\n",
    ")\n",
    "thresh = (\n",
    "    220 - 175 // 2,\n",
    "    135 - 90 // 2,\n",
    "    125 - 75 // 2\n",
    ")\n",
    "mask, img = threshold(img, color, thresh, sigma=.9)\n",
    "plot_img(img, fs=(14,14), title=\"Thresholded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RGB\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "erosion = cv2.erode(img, kernel,iterations = 1)\n",
    "plot_img(erosion, fs=(14,14), title=\"Erosion\")\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4,4))\n",
    "dilation = cv2.dilate(img, kernel,iterations = 1)\n",
    "plot_img(dilation, fs=(14,14), title=\"Dilation\")\n",
    "\n",
    "img = cv2.dilate(erosion, kernel, iterations = 1)\n",
    "plot_img(img, fs=(14,14), title=\"Erosion + Dilation\")\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, ksize=(5,5), sigmaX=3)\n",
    "plot_img(gray, fs=(14,14), title=\"Grayscale\")\n",
    "\n",
    "# use Canny edge detector to find edges in the image.  The thresholds determine how\n",
    "# weak or strong an edge will be detected.  These can be tweaked.\n",
    "lower_threshold = 100\n",
    "upper_threshold = 200\n",
    "edges = cv2.Canny(gray, lower_threshold, upper_threshold)\n",
    "plot_img(edges, fs=(14,14))\n",
    "\n",
    "# Mask top of image\n",
    "# edges[:425,:] = 0\n",
    "# edges[700:,:] = 0\n",
    "# edges[:,1150:] = 0\n",
    "plot_img(edges, fs=(14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hough Transform - Lines\n",
    "\n",
    "# detect lines in the image.  This is where the real work is done.  Higher threshold\n",
    "# means a line needs to be stronger to be detected, so again, this can be tweaked.\n",
    "thresh = 225\n",
    "lines = cv2.HoughLines(edges, 1, np.pi / 180, thresh)\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# convert each line to coordinates back in the original image\n",
    "for line in lines:\n",
    "    for rho, theta in line:\n",
    "        a = np.cos(theta)\n",
    "        b = np.sin(theta)\n",
    "        x0 = a * rho\n",
    "        y0 = b * rho\n",
    "        x1 = int(x0 + 1000 * -b)\n",
    "        y1 = int(y0 + 1000 * a)\n",
    "        x2 = int(x0 - 1000 * -b)\n",
    "        y2 = int(y0 - 1000 * a)\n",
    "\n",
    "        # draw each line on the image\n",
    "        cv2.line(img, pt1=(x1, y1), pt2=(x2, y2), color=(0, 0, 255), thickness=3)\n",
    "\n",
    "# write the image to disk\n",
    "plot_img(img, fs=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSV Color Thresh, Canny, Hough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSV\n",
    "\n",
    "img_fpath = os.path.join(IMG_DIR, 'volleyball_frame_00665.png')\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2HSV)\n",
    "#plot_img(img, fs=(14,14), title=\"Original\")\n",
    "\"\"\"\n",
    "H 0 - 10\n",
    "S = 100 - 150\n",
    "V = 175 - 220\n",
    "\"\"\"\n",
    "color = (\n",
    "    5,\n",
    "    50,\n",
    "    45\n",
    ")\n",
    "thresh = (\n",
    "    7,\n",
    "    256,\n",
    "    256\n",
    ")\n",
    "mask, img = threshold(img, color, thresh, sigma=1)\n",
    "#plot_img(img, fs=(14,14), title=\"Thresholded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RGB\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "erosion = cv2.erode(img, kernel,iterations = 1)\n",
    "plot_img(erosion, fs=(14,14), title=\"Erosion\")\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "dilation = cv2.dilate(img, kernel,iterations = 1)\n",
    "plot_img(dilation, fs=(14,14), title=\"Dilation\")\n",
    "\n",
    "img = cv2.dilate(img, kernel, iterations = 2)\n",
    "plot_img(img, fs=(14,14), title=\"Erosion + Dilation\")\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, ksize=(5,5), sigmaX=3)\n",
    "plot_img(gray, fs=(14,14), title=\"Grayscale\")\n",
    "\n",
    "# use Canny edge detector to find edges in the image.  The thresholds determine how\n",
    "# weak or strong an edge will be detected.  These can be tweaked.\n",
    "lower_threshold = 100\n",
    "upper_threshold = 200\n",
    "edges = cv2.Canny(gray, threshold1=lower_threshold, threshold2=upper_threshold, apertureSize=3)\n",
    "plot_img(edges, fs=(14,14))\n",
    "\n",
    "# Mask top of image\n",
    "plot_img(edges, fs=(14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HoughLinesP\n",
    "\n",
    "* Probabalistic Hough Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hough Transform - Lines\n",
    "\n",
    "x_delta = 100\n",
    "y_delta = 100\n",
    "min_line_length = 400\n",
    "max_line_gap = 2000\n",
    "thresh = 100\n",
    "#minLineLength - Minimum length of line\n",
    "#maxLineGap - Maximum allowed gap between line segments to treat them as single line.\n",
    "lines = cv2.HoughLinesP(edges, rho=1, theta=np.pi/180,\n",
    "                        threshold=thresh, maxLineGap=max_line_gap, \n",
    "                        minLineLength=min_line_length)\n",
    "line_coords = []\n",
    "\n",
    "for line in lines:\n",
    "    x1,y1,x2,y2 = line[0]\n",
    "    print(x1,y1,x2,y2)\n",
    "    if abs(x2 - x1) < x_delta:\n",
    "        orient = 'vertical'\n",
    "    elif abs(y2 - y1) < y_delta:\n",
    "        orient = 'horizontal'\n",
    "    else:\n",
    "        orient = 'other' #None\n",
    "        print('skipping line', x1,y1,x2,y2)\n",
    "    if orient is not None:\n",
    "        line_coords.append([x1,y1,x2,y2])\n",
    "        img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "        cv2.line(img, pt1=(x1, y1), pt2=(x2, y2), color=(0, 0, 255), thickness=3)\n",
    "        plot_img(img, fs=(18,18), title=coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HoughLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "thresh = 125\n",
    "lines = cv2.HoughLines(edges, 1, np.pi / 180, thresh)\n",
    "img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "\n",
    "line_coords = []\n",
    "\n",
    "max_y,max_x,chan = img.shape\n",
    "# convert each line to coordinates back in the original image\n",
    "for line in lines:\n",
    "    for p, theta in line:\n",
    "        # Stores the value of cos(theta) in a\n",
    "        a = np.cos(theta)\n",
    "        \n",
    "        # Stores the value of sin(theta) in b\n",
    "        b = np.sin(theta)\n",
    "        \n",
    "        # x0 stores the value p*cos(theta)\n",
    "        x0 = a * p\n",
    "        \n",
    "        # y0 stores the value p*sin(theta)\n",
    "        y0 = b * p\n",
    "        \n",
    "        # x1,x2,y1,y2 store rounded off values\n",
    "        x1 = int(x0 + 1000 * (-b))\n",
    "        y1 = int(y0 + 1000 * (a))\n",
    "        x2 = int(x0 - 1000 * (-b))\n",
    "        y2 = int(y0 - 1000 * (a))\n",
    "        \n",
    "        slope = (y2-y1)/(x2-x1)\n",
    "        # Filter out lines completely above the court\n",
    "        if y2 > 400 or y1 > 400:\n",
    "            # Label Horizontal / Vertical\n",
    "            if abs(slope) > 1:\n",
    "                orient = 'vertical'\n",
    "            else:\n",
    "                orient = 'horizontal'\n",
    "            coords = [x1,y1,x2,y2,p,theta,slope,orient]\n",
    "            line_coords.append(coords)\n",
    "            img = load_cv2_img(img_fpath, colorspace=cv2.COLOR_BGR2RGB)\n",
    "            cv2.line(img, pt1=(x1, y1), pt2=(x2, y2), color=(0, 0, 255), thickness=3)\n",
    "            plot_img(img, fs=(18,18), title=(x1,x2,y1,y2,p,theta,orient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the HoughTransform returns (p, and theta)\n",
    "for line in lines:\n",
    "    for p, radians in line:\n",
    "        print(line[0], p, radians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Horizontal / Vertical\n",
    "for line in line_coords:\n",
    "    if abs(line[2] - line[0]) < 200:\n",
    "        line.append('vertical')\n",
    "    elif abs(line[3] - line[1]) < 200:\n",
    "        line.append('horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1,y2,x2,y2,p,theta,orientation\n",
    "line_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up Hough Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://campushippo.com/lessons/detect-highway-lane-lines-with-opencv-and-python-21438a3e2\n",
    "* How to clean up the lines, extend them, and average nearby lines?\n",
    "* https://stackoverflow.com/questions/44449871/fine-tuning-hough-line-function-parameters-opencv\n",
    "* Since the lines are mostly vertical and horizontal, you can easily split the lines based on their position. If the two y-coordinates of one line are near each other, then the line is mostly horizontal. If the two x-coordinates are near each other, then the line is mostly vertical. So you can segment your lines into vertical lines and horizontal lines that way.\n",
    "* https://alyssaq.github.io/2014/understanding-hough-transform/\n",
    "\n",
    "ρ = x cos θ + y sin θ\n",
    "\n",
    "where:\n",
    "ρ (rho) = distance from origin to the line. [-max_dist to max_dist].\n",
    "          max_dist is the diagonal length of the image.  \n",
    "θ = angle from origin to the line. [-90° to 90°]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hough Transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_houghlines/py_houghlines.html\n",
    "* https://docs.opencv.org/3.3.1/d9/db0/tutorial_hough_lines.html\n",
    "* rho = \n",
    "* Returns lines in parametric (polar coordinates) form (p, theta)\n",
    "    * p = x cos(theta) + y sin(theta)\n",
    "    * p = perpendicular distance from origin (0,0) - top left corner\n",
    "    * theta = angle formed by p and horizontal axis (counter clockwise) (radians)\n",
    "    * r is measured in pixels and 0 is measured in radians.\n",
    "![Houghlines](https://docs.opencv.org/3.0-beta/_images/houghlines1.svg)\n",
    "![hough](http://cdncontribute.geeksforgeeks.org/wp-content/uploads/Hough_transform_diagram.png)\n",
    "\n",
    "* https://www.geeksforgeeks.org/line-detection-python-opencv-houghline-method/\n",
    "* Applications of Hough transform:\n",
    "\n",
    "    * It is used to isolate features of a particular shape within an image.\n",
    "    * Tolerant of gaps in feature boundary descriptions and is relatively unaffected by noise.\n",
    "    * Used extensively in barcode scanning, verification and recognition\n",
    "* Steps\n",
    "    * First parameter, Input image should be a binary image, so apply threshold edge detection before finding applying hough transform.\n",
    "    * Second and third parameters are r and θ(theta) accuracies respectively.\n",
    "    * Fourth argument is the threshold, which means minimum vote it should get for it to be considered as a line.\n",
    "    * Remember, number of votes depend upon number of points on the line. So it represents the minimum length of line that should be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harris Corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_cv2_img(img_fpath)\n",
    "gray = cv2.GaussianBlur(gray, ksize=(5,5), sigmaX=3)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "gray = np.float32(gray)\n",
    "\n",
    "dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "\n",
    "#result is dilated for marking the corners, not important\n",
    "dst = cv2.dilate(dst,None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst>0.05*dst.max()]=[0,0,255]\n",
    "\n",
    "plot_img(img, fs=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_cv2_img(img_fpath)\n",
    "#gray = cv2.GaussianBlur(gray, ksize=(5,5), sigmaX=3)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "gray = np.float32(gray)\n",
    "\n",
    "dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "dst = cv2.dilate(dst,None)\n",
    "ret, dst = cv2.threshold(dst,0.01*dst.max(),255,0)\n",
    "dst = np.uint8(dst)\n",
    "\n",
    "# find centroids\n",
    "ret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst)\n",
    "\n",
    "# define the criteria to stop and refine the corners\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "corners = cv2.cornerSubPix(gray,np.float32(centroids),(5,5),(-1,-1),criteria)\n",
    "\n",
    "# Now draw them\n",
    "res = np.hstack((centroids,corners))\n",
    "res = np.int0(res)\n",
    "img[res[:,1],res[:,0]]=[0,0,255]\n",
    "img[res[:,3],res[:,2]] = [0,255,0]\n",
    "\n",
    "plot_img(img, fs=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_cv2_img(img_fpath)\n",
    "gray = cv2.GaussianBlur(gray, ksize=(5,5), sigmaX=3)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret,thresh = cv2.threshold(gray, 127, 255, 0)\n",
    "image, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "img = cv2.drawContours(img, contours, -1, (0,255,0), 3)\n",
    "plot_img(img, fs=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.opencv.org/3.3.1/dd/d49/tutorial_py_contour_features.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Histogram\n",
    "\n",
    "# OpenCV - faster\n",
    "hist = cv2.calcHist(images=[img], channels=[0], mask=None, histSize=[256], ranges=[0,256])\n",
    "\n",
    "# Numpy\n",
    "hist, bins = np.histogram(img.ravel(), 256, [0,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Histogram\n",
    "plt.hist(img.ravel(), 256, [0,256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color histogram\n",
    "\n",
    "def plot_color_hist(img, bins=256, mask=None):\n",
    "    # Mask let's you select for certain regions\n",
    "    \n",
    "    color = ('b','g','r')\n",
    "    for i,col in enumerate(color):\n",
    "        histr = cv2.calcHist([img],[i],mask,[bins],[0,256])\n",
    "        plt.plot(histr, color=col)\n",
    "        plt.xlim([0,bins])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying a Mask\n",
    "\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "mask[400:700, 100:1100] = 255\n",
    "masked_img = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "plot_img(load_cv2_img(img_fpath))\n",
    "plot_img(mask)\n",
    "plot_img(masked_img)\n",
    "\n",
    "plot_color_hist(img, bins=30)\n",
    "plot_color_hist(img, bins=30, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSV Histogram - Hue holds the color information nicely\n",
    "img = cv2.imread(img_fpath)\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "hist = cv2.calcHist(images=[hsv], channels=[0], mask=None, histSize=[359], ranges=[0,359])\n",
    "plt.plot(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color Quantization\n",
    "# https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv.html\n",
    "\n",
    "img = cv2.imread(img_fpath)\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "Z = img.reshape((-1,3))\n",
    "\n",
    "# convert to np.float32\n",
    "Z = np.float32(Z)\n",
    "\n",
    "# define criteria, number of clusters(K) and apply kmeans()\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "K = 6\n",
    "ret, label, center = cv2.kmeans(Z,K,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "# Now convert back into uint8, and make original image\n",
    "center = np.uint8(center)\n",
    "res = center[label.flatten()]\n",
    "res2 = res.reshape((img.shape))\n",
    "\n",
    "plot_img(res2, fs=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_color_hist(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,w,c = img.shape\n",
    "minarea = h * w / 10\n",
    "gray = cv2.GaussianBlur(res2, ksize=(5,5), sigmaX=3)\n",
    "gray = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret,thresh = cv2.threshold(gray, 127, 255, 0)\n",
    "image, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "img = cv2.drawContours(img, contours, -1, (0,255,0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt in contours:\n",
    "    if cv2.contourArea(cnt) < minarea:\n",
    "        img = cv2.drawContours(img, [cnt], -1, (0,0,0), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(img, fs=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment with Color Range\n",
    "\n",
    "# Find the dominant color in the target region with masking\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "mask[600:650, 100:150] = 255\n",
    "masked_img = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "plot_img(load_cv2_img(img_fpath))\n",
    "plot_img(mask)\n",
    "plot_img(masked_img)\n",
    "\n",
    "plot_color_hist(img, bins=256)\n",
    "plot_color_hist(img, bins=256, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_fpath)\n",
    "img = cv2.GaussianBlur(img, ksize=(5,5), sigmaX=3)\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "court_color_bgr = np.uint8([[[75,100,190]]])\n",
    "hsv_court_color = cv2.cvtColor(court_color_bgr, cv2.COLOR_BGR2HSV)\n",
    "hue = hsv_court_color[0][0][0]\n",
    "hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define range of red color in HSV\n",
    "lower = np.array([hue-5,10,10])\n",
    "upper = np.array([hue+20,255,255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold the HSV image to get only blue colors\n",
    "mask = cv2.inRange(hsv, lower, upper)\n",
    " \n",
    "# Bitwise-AND mask and original image\n",
    "masked_img = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "plot_img(load_cv2_img(img_fpath), fs=(12,8))\n",
    "plot_img(mask, fs=(12,8))\n",
    "plot_img(masked_img, fs=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(masked_img, cv2.COLOR_BGR2GRAY)\n",
    "ret,thresh = cv2.threshold(gray, 127, 255, 0)\n",
    "image, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "img = cv2.drawContours(img, contours, -1, (0,255,0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(img, fs=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define range of red color in HSV\n",
    "lower = np.array([110,50,50])\n",
    "upper = np.array([130,255,255])\n",
    "\n",
    "# Threshold the HSV image to get only blue colors\n",
    "mask = cv2.inRange(hsv, lower, upper)\n",
    " \n",
    "# Bitwise-AND mask and original image\n",
    "masked_img = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "plot_img(load_cv2_img(img_fpath))\n",
    "plot_img(mask)\n",
    "plot_img(masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,w,c = img.shape\n",
    "minarea = h * w / 10\n",
    "img = cv2.imread(img_fpath)\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, ksize=(5,5), sigmaX=2)\n",
    "ret, thresh = cv2.threshold(gray, 127, 255, 0)\n",
    "image, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts = []\n",
    "for cnt in contours:\n",
    "    if cv2.contourArea(cnt) > minarea:\n",
    "        cnts.append(cnt)\n",
    "len(cnts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got it working!\n",
    "fig = plt.figure()\n",
    "axes = plt.axes([0, 0.03, 1, 0.97])\n",
    "\n",
    "img = plt.imread(fpaths[0])\n",
    "imgplot = axes.imshow(img, animated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_boxes = GT_BOXES\n",
    "def init():\n",
    "    return (imgplot,)\n",
    "\n",
    "def animate_w_boxes(fname):\n",
    "    fpath = os.path.join(IMG_DIR, fname)\n",
    "    img = plt.imread(fpath)\n",
    "    imgplot.set_array(img)\n",
    "    \n",
    "    # Remove old boxes\n",
    "    for p in axes.patches:\n",
    "        p.remove()\n",
    "    boxes = all_boxes[fname]\n",
    "    for box in boxes:\n",
    "        width = box['x2'] - box['x1']\n",
    "        height = box['y2'] - box['y1']\n",
    "        bb = Rectangle(\n",
    "            (box['x1'],box['y1']), \n",
    "            width, height,\n",
    "            fill=False,\n",
    "            edgecolor=\"red\")\n",
    "        axes.add_patch(bb)    \n",
    "    return (imgplot,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the animator. blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate_w_boxes, init_func=init,\n",
    "                               frames=fnames, interval=100, blit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~1 minute to general\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/finding-lane-lines-on-the-road-30cf016a1165\n",
    "* https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9\n",
    "* https://github.com/christopher5106/FastAnnotationTool\n",
    "* http://androidkt.com/train-object-detection/\n",
    "* https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md\n",
    "* https://towardsdatascience.com/building-a-real-time-object-recognition-app-with-tensorflow-and-opencv-b7a2b4ebdc32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "111px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
