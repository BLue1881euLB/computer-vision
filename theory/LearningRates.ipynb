{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LearningRates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"/home/bfortuner/workplace/VisionQuest\")\n",
    "from common import *\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn import metrics as scipy_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LearningRate():\n",
    "    def __init__(self, initial_lr, iteration_type):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.iteration_type = iteration_type #epoch or mini_batch\n",
    "\n",
    "    def get_learning_rate(self, optimizer):\n",
    "        return optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def set_learning_rate(self, optimizer, new_lr):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "\n",
    "    def adjust(self, optimizer, lr, iteration, params=None):\n",
    "        self.set_learning_rate(optimizer, lr)\n",
    "        return lr\n",
    "    \n",
    "def set_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def get_learning_rate(optimizer):\n",
    "    return optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SnapshotLR(LearningRate):\n",
    "    '''https://arxiv.org/abs/1704.00109'''\n",
    "    def __init__(self, initial_lr, iteration_type,\n",
    "                 max_lr, total_iters, n_cycles):\n",
    "        '''\n",
    "        n_iters = total number of mini-batch iterations during training\n",
    "        n_cycles = total num snapshots during training\n",
    "        max_lr = starting learning rate each cycle'''\n",
    "        super().__init__(initial_lr, iteration_type)\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iters = total_iters\n",
    "        self.cycles = n_cycles\n",
    "\n",
    "    def cosine_annealing(self, t):\n",
    "        '''t = current mini-batch iteration'''\n",
    "        return self.max_lr/2 * (math.cos(\n",
    "         (math.pi * (t % (self.total_iters//self.cycles))) /\n",
    "         (self.total_iters//self.cycles)) + 1)\n",
    "\n",
    "    def adjust(self, optimizer, iteration, params=None):\n",
    "        new_lr = self.cosine_annealing(iteration)\n",
    "        self.set_learning_rate(optimizer, new_lr)\n",
    "        return new_lr\n",
    "\n",
    "class DevDecayLR(LearningRate):\n",
    "    '''https://arxiv.org/abs/1705.08292'''\n",
    "    def __init__(self, initial_lr, iteration_type,\n",
    "                 decay_factor=0.9, decay_patience=1):\n",
    "        super().__init__(initial_lr, iteration_type)\n",
    "        self.decay_factor = decay_factor\n",
    "        self.decay_patience = decay_patience\n",
    "\n",
    "    def adjust(self, optimizer, iteration, params):\n",
    "        lr = super().get_learning_rate(optimizer)\n",
    "        best_iter = params['best_iter']\n",
    "\n",
    "        if (iteration - best_iter) > self.decay_patience:\n",
    "            print('Decaying learning rate by factor: {:.5f}'.format(\n",
    "                self.decay_factor).rstrip('0'))\n",
    "            lr *= self.decay_factor\n",
    "            super().set_learning_rate(optimizer, lr)\n",
    "        return lr\n",
    "\n",
    "class ScheduledLR(LearningRate):\n",
    "    def __init__(self, initial_lr, iteration_type, lr_schedule):\n",
    "        super().__init__(initial_lr, iteration_type)\n",
    "        self.lr_schedule = lr_schedule\n",
    "\n",
    "    def adjust(self, optimizer, iteration, params=None):\n",
    "        if iteration in self.lr_schedule:\n",
    "            new_lr = self.lr_schedule[iteration]\n",
    "        else:\n",
    "            new_lr = self.get_learning_rate(optimizer)\n",
    "        super().set_learning_rate(optimizer, new_lr)\n",
    "        return new_lr\n",
    "\n",
    "class DecayingLR(LearningRate):\n",
    "    def __init__(self, initial_lr, iteration_type, decay, n_epochs):\n",
    "         super().__init__(initial_lr, iteration_type)\n",
    "         self.decay = decay\n",
    "         self.n_epochs = n_epochs\n",
    "\n",
    "    def exponential_decay(self, iteration, params=None):\n",
    "        '''Update learning rate to `initial_lr` decayed\n",
    "        by `decay` every `n_epochs`'''\n",
    "        return self.initial_lr * (self.decay ** (iteration // self.n_epochs))\n",
    "\n",
    "    def adjust(self, optimizer, iteration):\n",
    "        new_lr = self.exponential_decay(iteration)\n",
    "        super().set_learning_rate(optimizer, new_lr)\n",
    "        return new_lr\n",
    "\n",
    "class CyclicalLR(LearningRate):\n",
    "    '''https://arxiv.org/abs/1506.01186'''\n",
    "    def __init__(self, initial_lr, iteration_type, n_iters, cycle_length,\n",
    "                 min_lr, max_lr):\n",
    "         assert initial_lr == min_lr\n",
    "         super().__init__(initial_lr, iteration_type)\n",
    "         self.n_iters = n_iters\n",
    "         self.cycle_length = cycle_length\n",
    "         self.min_lr = min_lr\n",
    "         self.max_lr = max_lr\n",
    "\n",
    "    def triangular(self, iteration):\n",
    "        iteration -= 1 # if iteration count starts at 1\n",
    "        cycle = math.floor(1 + iteration/self.cycle_length)\n",
    "        x = abs(iteration/(self.cycle_length/2) - 2*cycle + 1)\n",
    "        new_lr = self.min_lr + (self.max_lr - self.min_lr) * max(0, (1-x))\n",
    "        return new_lr\n",
    "\n",
    "    def adjust(self, optimizer, iteration, best_iter=1):\n",
    "        new_lr = self.triangular(iteration)\n",
    "        super().set_learning_rate(optimizer, new_lr)\n",
    "        return new_lr\n",
    "\n",
    "def cosine_annealing(lr_max, T, M, t):\n",
    "    '''\n",
    "    t = current mini-batch iteration\n",
    "    # lr(t) = f(t-1 % T//M)\n",
    "    # lr(t) = lr_max/2 * (math.cos( (math.pi * (t % T/M))/(T/M) ) + 1)\n",
    "    '''\n",
    "    return lr_max/2 * (math.cos( (math.pi * (t % (T//M)))/(T//M)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
